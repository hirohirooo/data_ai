{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNawZNsK1IApTy8Wrr3BIJI"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# データシステムの知能化とデザイン\n","# 第4回課題\n","## 62014205\n","## 中野宏志"],"metadata":{"id":"A5BA0fHWIe4F"}},{"cell_type":"markdown","source":["#活性化関数\n","\n","NNにおいて、線形変換のみでは非線形な分布に対して適切な表現をすることができないため、各層で線形変換に引き続いて非線形変換を施すことで全体の関数が非線形性を持つようにするという工夫が施されている。この変換を行う関数を特に活性化関数と呼ぶ。\n","活性化関数には具体的にはReLU関数、シグモイド関数、tanh関数、Linear関数、logistic関数、softmax関数が挙げられる。\n","\n","現在最も使用されているなReLU関数はシンプルで演算が高速、正の入力値では微分値が1固定で入力に応じて値を大きくできるため勾配消失が起こりにくいというメリットがある。一方、x=0で不連続部分不可能であることを基本的に無視していることや、負の場合は終息しにくいというデメリットもある。\n","\n","#損失関数\n","\n","出力と正解の間の誤差を定義する関数のことを表し、この損失の値を最小化、最大化することで、機械学習モデルを最適化する。損失関数の例として平均二乗誤差(MSE)などが挙げられる。\n","\n","例えば機械学習の一手法であるニューラルネットワークでは、損失関数は誤差逆伝播法（バックプロパゲーション：Back-propagation）と呼ばれる最適化の処理で用いられる。ちなみに誤差逆伝播法では、損失関数は誤差関数（Error function）とも呼ばれる。\n","\n","\n","#SGD\n","\n","SGDとは確率的勾配降下法と呼ばれるミニバッチ学習の一種である。ニューラネットワークを勾配降下法で最適化する場合、計算量削減のため、データを1つ1つ用いてパラメータを更新することは行わず、いくつかのデータをまとめて入力し、それぞれの勾配を計算した後に、その勾配の平均値を用いてパラメータの更新を行う。\n","\n","具体的には、学習データセットからk(>0)個のデータを抽出し、そのk個のデータに対する目的関数の平均値を小さくするようにパラメータを更新するという作業を繰り返すものである。ｋをミニバッチサイズと呼びこのような学習方法がSGDと呼ばれている。\n","\n","SGDを用いることで、計算時間が劇的に少なくできるだけでなく、凸関数でなかった場合であっても、適当な条件のもとでほぼ確実に局所最適解に収束することが知られている。\n","\n","#Adam\n","\n","最適化アルゴリズムの一つで最も一般的に用いられるものである。学習の進行度合いや過去の履歴に基づいて更新量を調整する方法で、モーメンタムとRMSPropの利点を合わせた手法である。具体的には勾配の移動平均と勾配の2乗の移動平均であり、それぞれモーメンタムおよびRMSPropの構成式に他ならない。パラメータも多いが、これらのパラメータについては提案者により妥当な値が提案されている。\n","\n","しかしAdamの場合、特定のバッチに有用な情報がたまたま集中していた場合、その情報は更新計算でかき消されてしまう、つまり良いデータは取っておきたいということができないというデメリットもある。このため改良版のAMSGradが提案されており、PyTorchなどもサポートしている。\n","\n","参考）\n","\n","https://ai-trend.jp/basic-study/neural-network/activation_function/\n","\n","https://atmarkit.itmedia.co.jp/ait/articles/2104/15/news030.html\n"],"metadata":{"id":"jYptqnWFImF3"}}]}